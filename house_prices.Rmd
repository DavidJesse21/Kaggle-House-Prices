---
title: "Kaggle House Prices"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE
)
```

```{r libraries, include=FALSE}
# data wrangling
library(dplyr)
library(tidyr)
library(purrr)
library(stringr)

# data visualisation
library(ggplot2)
library(scales) #formatting numbers
library(corrr) #correlation dataframes and plots
library(vip) # variable importance plots

# machine learning
library(recipes)
library(parsnip)
library(tune)
library(finetune)
library(rsample)
library(dials)
library(workflows)
library(yardstick)
library(broom)

# nice table output
library(kableExtra)
```

```{r other options, include=FALSE}
# theme for ggplot
theme_set(theme_minimal())

# turn off scientific notation
options(scipen=999)
```



# 1. Data and Initial EDA


## 1.1 Data

First we need to read in the data.
Since there are some unhandy column names which begin with [0-9] I am also going to use the
`clean_names()` function from the `{janitor}` package to deal with that issue.

```{r}
# read data
df <- vroom::vroom("data/train.csv")
df_predict <- vroom::vroom("data/test.csv")

# clean column names (especially those beginning with [0-9])
df <- janitor::clean_names(df)
df_predict <- janitor::clean_names(df_predict)
```


## 1.2 General Overview of the Data

Next, it is a good idea to get a general overview of the data.
This includes questions like:

* What is the dimension of the data (how many rows and columns)?
* What data types do the features have?
* Are missing values present? If yes, where and how many?

```{r, include = FALSE}
head(df, 3)
```

```{r}
# dimension and data types
skimr::skim(df) %>%
  summary()
```

```{r, echo=FALSE}
# EDA on missing values
# output not shown because it's so long
skimr::skim(df) %>%
  select(skim_type, skim_variable, n_missing, complete_rate)
```
Some takeaways regarding the occurrence of missing values:

- Most features do not have missing values
- Features with lots of missing values are:
  - `alley`
  - `fireplace_qu`
  - `fence`
  - `pool_qc `
  - `misc_feature`
- We can assume that the values of the above mentioned features are not missing at random
- Other features with missing values are usually missing in around 5% of the cases



## 1.3 Target Variable

For regression tasks it is always a good idea to take a look at the target variable and see
how it is distributed.

```{r}
# original distribution of target variable
df %>%
  ggplot(aes(sale_price)) +
  geom_histogram(color = "black", fill = "#1E88E5") +
  scale_x_continuous(labels = comma) +
  labs(
    x = "Sale Price",
    y = " ",
    title = 'Distribution of "Sale Price" Target Variable'
  ) +
  theme(
    plot.title = element_text(
      size = 16,
      margin = margin(0, 0, 20, 0)
      ),
    plot.title.position = "plot",
    axis.title.x = element_text(
      margin = margin(10, 0, 0, 0)
    )
  )
```

From  this plot you can tell that the target variable is distributed right skewed.
For prediction purposes it is usually helpful when the distribution is (approximately) normal.
This can be achieved by some transformation, e.g. a logarithmic one.
Also the competition rules require you to predict the logarithmic price as well.<br>
Let's create the same plot again with a logarithmic transformation before.

```{r}
# distribution of target variable after logarithmic transformation
df %>%
  mutate(sale_price = log(sale_price)) %>%
  ggplot(aes(sale_price)) +
  geom_histogram(color = "black", fill = "#1E88E5") +
  scale_x_continuous(labels = comma) +
  labs(
    x = "Sale Price",
    y = " ",
    title = 'Distribution of "Sale Price" after logarithmic Transformation'
  ) +
  theme(
    plot.title = element_text(
      size = 16,
      margin = margin(0, 0, 20, 0)
      ),
    plot.title.position = "plot",
    axis.title.x = element_text(
      margin = margin(10, 0, 0, 0)
    )
  )
```

This comes much closer to a normal distribution and will very likely help us in making better predictions.

```{r}
# apply log transformation in data
df  <- df %>%
  mutate(sale_price = log(sale_price))
```



# 2. EDA on numeric Features

Let's explore and analyse the data more closely.
Since this is a regression task and there are so many numeric features, it is probably a good idead to start exploring those.<br>
Let's get a firs overview of all numeric features:

```{r}
# column names of numeric features
df %>%
  select(-sale_price) %>%
  select_if(is.numeric) %>%
  names()
```

`id` is not an actual feature and can be omitted.<br>
Additionally `ms_sub_class` has numeric values but is not a numeric feature (look at the description file). So this needs to be changed first.


```{r}
# initial correlation dataframe
corr_df <- df %>%
  select(-id) %>%
  select_if(is.numeric) %>%
  relocate(sale_price) %>%
  correlate(diagonal = 1)

# sort by correlation to `sale_price` and select
# 10 most important
corr_df <- corr_df %>%
  arrange(desc(sale_price)) %>%
  head(11)

corr_vars <- unique(corr_df$term)

corr_df <- corr_df %>%
  select(c("term", corr_vars)) %>%
    mutate_if(
    is.numeric,
    ~ na_if(., . == 1)
  ) %>%
  shave()

# plot
corr_plot <- corr_df %>%
  rplot(
    print_cor = TRUE,
    colors = c("#D81B60", "white", "#1E88E5")
  )

corr_plot +
  labs(title = 'Correlations to "Sale Price"') +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 0.5),
    plot.title = element_text(
      size = 16,
      margin = margin(0, 0, 20, 0)
      ),
    plot.title.position = "plot"
  )
```

Based on these first insights it is probably a good idead to do some further EDA on some of the numeric features.


### 2.1 Overall Quality

```{r}
# EDA on `overall_qual`
df %>%
  ggplot(aes(x = overall_qual, y = sale_price)) +
  geom_point(
    na.rm = TRUE,
    position = "jitter",
    color = "#6C6C6D",
    alpha = 0.5
    ) +
  scale_x_continuous(breaks = 1:10) +
  geom_smooth(
    method = "lm",
    na.rm = TRUE,
    color = "#1E88E5",
    se = FALSE,
    size = 1.4
    ) +
  labs(
    x = "Overall Quality Rating",
    y = "Sale Price"
  ) +
  theme(
    axis.title.x = element_text(margin = margin(10, 0, 0, 0)),
    axis.title.y = element_text(margin = margin(0, 10, 0, 0))
  )
```


### 2.2 Ground Living Area

```{r}
# EDA on `gr_liv_area`

# for annotation in plot
cor_gr_liv1 = cor(df$gr_liv_area, df$sale_price, use = "pairwise.complete.obs")

df %>%
  ggplot(aes(x = gr_liv_area, y = sale_price)) +
    geom_point(
    na.rm = TRUE,
    position = "jitter",
    color = "#6C6C6D",
    alpha = 0.5
    ) +
  geom_smooth(
    method = "lm",
    na.rm = TRUE,
    color = "#1E88E5",
    se = FALSE,
    size = 1.4
    ) +
  labs(
    x = "Ground Living Area",
    y = "Sale Price"
  ) +
  theme(
    axis.title.x = element_text(margin = margin(10, 0, 0, 0)),
    axis.title.y = element_text(margin = margin(0, 10, 0, 0))
  ) +
  annotate(
    x = 5000, y = 11,
    label = paste0("Correlation: ", round(cor_gr_liv1, 2)),
    geom = "text", size = 4.5
  )
```

In this plot you can see that the relation between `gr_liv_area` and `sale_price` is quite strong but highly influenced by some outliers.
You can remove these outliers from the training data but that might not necessarily be a good idea since the machine learning model still needs to be robust to these observations as well in the test data.
Another approach is to use some sort of transformation of `gr_liv_area`.<br>
More precisely I am going to look at the effect of a *Box-Cox Transformation* and compare the resulting correlation coefficient to the previous one (without any transformation).


```{r}
# EDA on `gr_liv_area`with Box-Cox Transformation before

# BoxCox transformation of `gr_liv_area`
df_bc <- recipe(sale_price ~ ., data = select(df, gr_liv_area, sale_price)) %>%
  step_BoxCox(all_numeric(), -sale_price) %>%
  prep() %>%
  juice()

# correlation after transformation
cor_gr_liv2 <- cor(df_bc$gr_liv_area, df_bc$sale_price, use = "pairwise.complete.obs")

# plot and explore effect of transformation
df_bc %>%
  ggplot(aes(x = gr_liv_area, y = sale_price)) +
    geom_point(
    na.rm = TRUE,
    position = "jitter",
    color = "#6C6C6D",
    alpha = 0.5
    ) +
  geom_smooth(
    method = "lm",
    na.rm = TRUE,
    color = "#1E88E5",
    se = FALSE,
    size = 1.4
    ) +
  labs(
    x = "Ground Living Area",
    y = "Sale Price"
  ) +
  theme(
    axis.title.x = element_text(margin = margin(10, 0, 0, 0)),
    axis.title.y = element_text(margin = margin(0, 10, 0, 0))
  ) +
  annotate(
    x = 8.5, y = 11,
    label = paste0("Correlation: ", round(cor_gr_liv2, 2)),
    geom = "text", size = 4.5
  )
```

Applying the *Box-Cox transformation* increased the correlation between `gr_liv_area` and `sale_price` by 0.03.
While this might not sound like a lot you still need to keep in mind that there are various other area related features which probably follow the same pattern (i.e. that there are some extreme observations).
So applying the transformation to all of those features and maybe removing some outliers before that might help a lot.



### 2.3 Identify area features

There are a lot of area related features which are very likely not independent of each other.
In some cases one feature might even be a linear combination of some other features.
We need to find a way to deal with these features.

```{r}
# extract list of area related features ---

# list of feature names
li_features <- names(select(df, -id, -sale_price))

# detect area related feature
li_features_area <- c(
  li_features[str_detect(li_features, "area")],
  li_features[str_detect(li_features, "sf")]
)

li_features_area
```

##### Basement features

```{r}
# basement features ---
df %>%
  transmute(
    total_bsmt_sf = total_bsmt_sf,
    total_bsmt_2 = bsmt_fin_sf1 + bsmt_fin_sf2 + bsmt_unf_sf
  ) %>%
  summarise(correlation = cor(total_bsmt_sf, total_bsmt_2, use = "pairwise.complete.obs"))
```

As you can see `total_bsmt_sf` is a linear combination of the other basement features.
However, we should not simply exclude the other basement features, since they contain information about the quality of the basement area.
Therefore we can use them to calculate shares/ratios based on the total basement area.


##### Square Feet Living Area above Ground

```{r}
li_features_area[
  !str_detect(li_features_area, "bsmt")
]
```


```{r}
# living area features
df %>%
  select(gr_liv_area, ends_with("flr_sf"), low_qual_fin_sf) %>%
  transmute(
    gr_liv_area = gr_liv_area,
    gr_total = rowSums(select(., -gr_liv_area))
  ) %>%
  summarise(
    correlation = cor(gr_liv_area, gr_total, use = "pairwise.complete.obs")
  )
```

The same goes for the livng area features.
They are perfectly collinear but also do contain some qualitative information which might impact the effects.



### 2.4 Count Features (rooms)

It might also be worth mentioning the bathroom features, which are the following:

```{r}
names(df)[str_detect(names(df), "bath")]
```

Instead of having four different features which tell very similar things, we can simply combine them by calculating the total number of bathrooms.
Full baths are going to be weighted normally and half baths are going to be weighted 0.5.





# 3. EDA on categorical Features

Since for categorical features we cannot calculate simple correlations we need to pick a different approach to get a brief overview of the different and most important categorical features.
My approach here is to fit a random forest model with categorical features and only and after that look at the variable importances in a plot.

```{r}
# fit random forest with categorical features only ---

rf_cat <- rand_forest(trees = 250) %>%
  set_mode("regression") %>%
  set_engine("ranger", importance = "impurity")

rf_dat <- df %>%
  select_if(~!is.numeric(.)) %>%
  cbind(sale_price = df$sale_price) %>%
  mutate_at(
    vars(-sale_price),
    ~replace_na(., "missing")
  )

rf_cat_fit <- rf_cat %>%
  fit(
    sale_price ~ .,
    data = rf_dat
  )
```

```{r}
# vip plot of categorical random forest ---
vip(rf_cat_fit, num_features = 15) +
  labs(title = "Importance of categorical Features") +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(
      margin = margin(0, 0, 10, 0)
    )
  )
```

Most of the most important categorical are ordinal features associated with a rating (e.g. kitchen_qual).
Some of them have an associated numeric feature and hence can be combined to generate interaction terms:

- `bsmt_qual:total_bsmt_sf`
- `garage_type:garage_area/cars`

Furthermore, I think that the feature `neighborhood` could be quite interesting regarding interaction terms as well.
I.e. I assume that the price of one additional square foot of living area varies among the neighborhoods/districts of the city.

```{r}
df %>%
  ggplot(aes(x = sale_price, y = reorder(neighborhood, sale_price))) +
  geom_boxplot() +
  labs(
    x = "Price",
    y = "Neighborhood / District",
    title = "Sale Price Variation by Neighborhood"
  ) +
  theme(
    plot.title.position = "plot",
    plot.title = element_text(
      margin = margin(0, 0, 10, 0)
    )
  )
```

You can clearly tell that `neighborhood` is an important feature, too.
But still, numeric features or those that come closer to that (i.e. ordinal features) are more important for predictions.
The combination of both of them might improve results even further:

- `neighborhood:gr_liv_area`
- `neighborhood:lot_area`


# 4. Year and Date Features

```{r}
# names of year and date features
paste0(
  "Year and Date Features:\n",
  paste0(
    c(
      names(df)[str_detect(names(df), "yr")],
      names(df)[str_detect(names(df), "year")],
      names(df)[str_detect(names(df), "mo_")]
    ),
    collapse = ", "
  )
) %>% cat()
```
From the features `garage_yr_blt` and `year_built` the age of the house/garage can be calculated and thus numeric features can be derived from them.
This is not the same for `yr_sold` and `mo_sold`.
For example there is no logical reasoning like December is more/better than January.
Because of that these two features are going to be converted to factor variables.

Then there is the feature `year_remod_add`.
It is the remodel date but according to the documentation the same like `year_built` if it has not been remodelled.
I think the samrtest decision is just so extract the information if the house has been remodelled or not.

```{r}
# number of houses that have been remodelled
df %>%
  transmute(has_remod = ifelse(year_built == year_remod_add, "No", "Yes")) %>%
  count(has_remod)
```








```{r}
df %>%
  select_if(~!is.numeric(.)) %>%
  bind_cols(sale_price = df$sale_price) %>%
  mutate_all(~replace_na(., "missing"))

```

```{r}
df %>%
  select_if(~!is.numeric(.)) %>%
  cbind(sale_price = df$sale_price) %>%
  mutate_at(
    vars(-sale_price),
    ~replace_na(., "missing")
  )
```






